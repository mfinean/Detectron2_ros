#!/usr/bin/env python
import sys
import threading
import time

import cv2 as cv
import numpy as np
import rospy
from centermask.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest, CameraInfo
import pyrealsense2 as rs2

# from detectron2_ros.msg import PersonPositions, PersonPosition
from finean_msgs.msg import PersonPositions, PersonPosition

# For sync depth and rgb
import message_filters


# Needed to update backbone registry
import centermask.modeling.backbone

class Centermask2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        pos = PersonPositions()

        self._bridge = CvBridge()
        self._last_rgb_msg = None
        self._last_depth_msg = None
        self._msg_lock = threading.Lock()
        self._image_counter = 0

        self.score_thresh = 0.60
        self.removal_classes = ['person', 'chair']

        self.cfg = get_cfg()
        self.cfg.merge_from_file(self.load_param('~config'))
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param('~detection_threshold') # set threshold for this model
        self.cfg.MODEL.WEIGHTS = self.load_param('~model')
        self.predictor = DefaultPredictor(self.cfg)
        self._class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get("thing_classes", None)

        self._visualization = self.load_param('~visualization',True)
        self._result_pub = rospy.Publisher('~result', Result, queue_size=1)
        self._vis_pub = rospy.Publisher('~visualization', Image, queue_size=1)
        self._static_depth_img_pub = rospy.Publisher('~static_depth_image', Image, queue_size=1)
        self._static_depth_info_pub = rospy.Publisher(self.load_param('~static_depth_camera_info'), CameraInfo, queue_size=1)
        self.camera_info_msg   = None

        self._person_pos_pub = rospy.Publisher(self.load_param('~person_positions'), PersonPositions, queue_size=1)
        self.getIntrinsics()

        # self.getManualHSRIntrinsics()
        # self.getManualRealsenseIntrinsics()

        self._image_sub = message_filters.Subscriber(self.load_param('~rgb_input'), Image)
        self._depth_sub = message_filters.Subscriber(self.load_param('~depth_input'), Image)

        self.ts = message_filters.ApproximateTimeSynchronizer([self._image_sub, self._depth_sub], 1, 0.003)
        self.ts.registerCallback(self.callback_images)

        self.start_time = time.time()
        rospy.loginfo("Centermask2 Node Initialized")

    def convertPixelToPosition(self, depth_img, mask):
        # Get the pixel location and depth of person centroid
        count = (mask == 1).sum()

        y_center, x_center = np.argwhere(mask==True).sum(0)/count
        depth = np.median(depth_img[mask.astype(bool)])

        # Convert to 3D position in camera coords 
        xyz = rs2.rs2_deproject_pixel_to_point(self.intrinsics, [x_center, y_center], depth/1000.0)
                    
        person_position_msg = PersonPosition()
        person_position_msg.x = xyz[0]
        person_position_msg.y = xyz[1]
        person_position_msg.z = xyz[2]

        # return xyz
        return person_position_msg

    def getManualHSRIntrinsics(self):
        rospy.loginfo("Updating camera info")

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = 640
        self.intrinsics.height = 480
        self.intrinsics.ppx = 322.2156457958147
        self.intrinsics.ppy = 238.8396597454201
        self.intrinsics.fx = 537.3372465571922
        self.intrinsics.fy = 537.8279138435478
        self.intrinsics.model = rs2.distortion.brown_conrady

    def getManualRealsenseIntrinsics(self):
        rospy.loginfo("Updating camera info")

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = 640
        self.intrinsics.height = 480
        self.intrinsics.ppx = 324.9241638183594
        self.intrinsics.ppy = 236.38864135742188
        self.intrinsics.fx = 618.6040649414062
        self.intrinsics.fy = 618.9261474609375
        self.intrinsics.model = rs2.distortion.brown_conrady

    def getIntrinsics(self):
        # Get camera info
        rospy.loginfo("Updating camera info")
        cameraInfo = rospy.wait_for_message(self.load_param('~camera_info'), CameraInfo, 60)
        
        self.camera_info_msg = cameraInfo

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = cameraInfo.width
        self.intrinsics.height = cameraInfo.height
        self.intrinsics.ppx = cameraInfo.K[2]
        self.intrinsics.ppy = cameraInfo.K[5]
        self.intrinsics.fx = cameraInfo.K[0]
        self.intrinsics.fy = cameraInfo.K[4]
        self.intrinsics.model = rs2.distortion.brown_conrady
        rospy.loginfo("Camera intrinsics set.")

    def run(self):

        if self._msg_lock.acquire(False):
            img_msg = self._last_rgb_msg
            depth_msg = self._last_depth_msg
            self._last_rgb_msg = None
            self._last_depth_msg = None
            self._msg_lock.release()
        else:
            return

        self._image_counter = self._image_counter + 1
        if (self._image_counter % 11) == 10:
            rospy.loginfo("Images detected per second=%.2f",
                            float(self._image_counter) / (time.time() - self.start_time))

        # Convert images
        np_image = self.convert_to_cv_image(img_msg)
        np_depth_image = self.convert_to_cv_image(depth_msg)

        # Get the masks and objects seen
        outputs = self.predictor(np_image)
        result = outputs["instances"].to("cpu")
        
        result_msg = self.getResult(result)
        
        # Publish summary results
        self._result_pub.publish(result_msg)
        
        # Mask out people with score greater than threshold
        class_ids = result.pred_classes if result.has("pred_classes") else None
        class_names = np.array(self._class_names)[class_ids.numpy()]        
        retain_inds = (result.scores.numpy() > self.score_thresh) & (class_names == 'person')
        result = result[retain_inds]

        # For each mask
        static_depth_img = np_depth_image.copy()
        kernel = np.ones((5,5),np.uint8)

        num_inds = sum(retain_inds)
        # print("Number of masks: {0}".format(num_inds))
        all_person_positions_msg = PersonPositions()
        all_person_positions_msg.header = depth_msg.header
        if num_inds > 0:

            masks = np.asarray(result.pred_masks)
            for i in range(num_inds):
                mask = masks[i]
                person_position_msg = self.convertPixelToPosition(np_depth_image, mask)
                all_person_positions_msg.positions.append(person_position_msg)

                # Get the static depth image 
                static_depth_img[(cv.dilate(np.float32(mask),kernel,iterations = 2)).astype(bool)]=0

        static_depth_image_msg = self._bridge.cv2_to_imgmsg(static_depth_img)
        static_depth_image_msg.header = depth_msg.header
        self.camera_info_msg.header = depth_msg.header

        self._static_depth_img_pub.publish(static_depth_image_msg)
        self._static_depth_info_pub.publish(self.camera_info_msg)

        # print("Person {0} at {1}".format(i, person_pos))

        # rospy.loginfo("Publishing person position")
        self._person_pos_pub.publish(all_person_positions_msg)        


        # Visualize results
        if self._visualization:
            v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)
            v = v.draw_instance_predictions(result)
            img = v.get_image()[:, :, ::-1]

            image_msg = self._bridge.cv2_to_imgmsg(img)
            self._vis_pub.publish(image_msg)

    def getResult(self, predictions):

        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._rgb_header
        result_msg.class_ids = predictions.pred_classes if predictions.has("pred_classes") else None
        result_msg.class_names = np.array(self._class_names)[result_msg.class_ids.numpy()]
        result_msg.scores = predictions.scores if predictions.has("scores") else None

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]]=255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    def getThresholdedResult(self, predictions):

        inds = np.where(predictions.scores.numpy() > self.score_thresh)[0]

        boxes = predictions.pred_boxes[inds] if predictions.has("pred_boxes") else None
        

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._rgb_header
        result_msg.class_ids = predictions.pred_classes[inds] if predictions.has("pred_classes") else None
        result_msg.class_names = np.array(self._class_names)[result_msg.class_ids.numpy()][inds]
        result_msg.scores = predictions.scores[inds] if predictions.has("scores") else None

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]]=255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    def convert_to_cv_image(self, image_msg):

        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ['rgb8', 'bgr8']:
            encoding = np.uint8
        elif image_msg.encoding.lower() == 'mono8':
            encoding = np.uint8
        elif image_msg.encoding.lower() == '16uc1':
            # For depth images        
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == '32fc1':
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(shape=(image_msg.height, image_msg.width, channels),
                            dtype=encoding, buffer=image_msg.data)

        if image_msg.encoding.lower() == 'mono8':
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == '16uc1':
            pass
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def getStaticDepthImage(self, depth_image, predictions):

        static_depth_img = depth_image.copy()
        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return static_depth_img # No masks found so return the whole depth image

        class_ids = predictions.pred_classes if predictions.has("pred_classes") else None
        class_names = np.array(self._class_names)[class_ids.numpy()]
        scores = predictions.scores if predictions.has("scores") else None
        
        # arr = (class_names == 'person') & (scores.numpy() > 0.5)
        arr = (np.isin(class_names, self.removal_classes)) & (scores.numpy() > 0.6)
        inds = np.where(arr)[0]
        
        if len(inds)<1:
            return static_depth_img # No relevant masks found so return whole depth image
        
        kernel = np.ones((5,5),np.uint8)

        for i, (x1, y1, x2, y2) in enumerate(boxes[arr]):
            # static_depth_img[masks[inds[i]]]=0
            # Could be worth transforming the masks to both reduce noise and dilate
            # static_depth_img[cv.dilate(masks[inds[i]],kernel,iterations = 1) ]=0
            static_depth_img[(cv.dilate(np.float32(masks[inds[i]]),kernel,iterations = 2)).astype(bool)]=0

        return static_depth_img

    def callback_images(self, rgb_msg, depth_msg):
        # rospy.logdebug("Get time-synchronised images")
        # rospy.loginfo("Got time-synchronised images")
        if self._msg_lock.acquire(False):
            self._last_rgb_msg = rgb_msg
            self._last_depth_msg = depth_msg
            self._rgb_header = rgb_msg.header
            self._depth_header = depth_msg.header
            self._msg_lock.release()

            self.run()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Centermask2] %s: %s", param, new_param)
        return new_param

def main(argv):
    rospy.init_node('centermask2_ros')
    node = Centermask2node()
    rospy.spin()

if __name__ == '__main__':
    main(sys.argv)
