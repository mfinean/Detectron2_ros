#!/usr/bin/env python
import sys
import threading
import time

import cv2 as cv
import numpy as np

from centermask.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest, CameraInfo
import pyrealsense2 as rs2

# from detectron2_ros.msg import PersonPositions, PersonPosition
from finean_msgs.msg import PersonPositions, PersonPosition

# For sync depth and rgb
import message_filters


# Needed to update backbone registry
import centermask.modeling.backbone

class KTPAnalyser(object):
    def __init__(self):
        print("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self.score_thresh = 0.50
        self.removal_classes = ['person']

        self.cfg = get_cfg()
        self.cfg.merge_from_file("$(find detectron2_ros)/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model
        self.cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
        self.predictor = DefaultPredictor(self.cfg)
        self._class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get("thing_classes", None)
        
        self.getManualKinectIntrinsics()

        print("Centermask2 Node Initialized")

    def convertPixelToPosition(self, depth_img, mask):
        # Get the pixel location and depth of person centroid
        count = (mask == 1).sum()

        y_center, x_center = np.argwhere(mask==True).sum(0)/count
        depth = np.median(depth_img[mask.astype(bool)])

        # Convert to 3D position in camera coords 
        xyz = rs2.rs2_deproject_pixel_to_point(self.intrinsics, [x_center, y_center], depth/1000.0)
                    
        # return xyz
        return xyz

    def getManualKinectIntrinsics(self):
        # D: [0.0, 0.0, 0.0, 0.0, 0.0]
        # K: [525.0, 0.0, 319.5, 0.0, 525.0, 239.5, 0.0, 0.0, 1.0]
        # R: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
        # P: [525.0, 0.0, 319.5, 0.0, 0.0, 525.0, 239.5, 0.0, 0.0, 0.0, 1.0, 0.0]


        rospy.loginfo("Updating camera info")

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = 640
        self.intrinsics.height = 480
        self.intrinsics.ppx = 319.5
        self.intrinsics.ppy = 239.5
        self.intrinsics.fx = 525.0
        self.intrinsics.fy = 525.0
        self.intrinsics.model = rs2.distortion.brown_conrady

    def run(self, rgb_file, depth_file, robot_pose):

        # Convert images
        np_image = cv.imread(rgb_file)
        np_depth_image = cv.imread(depth_file)

        # Get the masks and objects seen
        outputs = self.predictor(np_image)
        result = outputs["instances"].to("cpu")
        
        result_msg = self.getResult(result)
        
        # Mask out people with score greater than threshold
        class_ids = result.pred_classes if result.has("pred_classes") else None
        class_names = np.array(self._class_names)[class_ids.numpy()]        
        retain_inds = (result.scores.numpy() > self.score_thresh) & (class_names == 'person')
        result = result[retain_inds]

        # For each mask
        static_depth_img = np_depth_image.copy()
        kernel = np.ones((5,5),np.uint8)

        num_inds = sum(retain_inds)
        print("Number of masks: {0}".format(num_inds))
        all_person_positions_msg = PersonPositions()
        all_person_positions_msg.header = depth_msg.header
        if num_inds > 0:
            masks = np.asarray(result.pred_masks)
            for i in range(num_inds):
                mask = masks[i]
                person_position = self.convertPixelToPosition(np_depth_image, mask)

        print("Allpositions extracted...")
        
    def getResult(self, predictions):

        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._rgb_header
        result_msg.class_ids = predictions.pred_classes if predictions.has("pred_classes") else None
        result_msg.class_names = np.array(self._class_names)[result_msg.class_ids.numpy()]
        result_msg.scores = predictions.scores if predictions.has("scores") else None

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]]=255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Centermask2] %s: %s", param, new_param)
        return new_param

def main(argv):

    # Load the rgb, depth, robot_poses




    node = Centermask2node()
    rospy.spin()

if __name__ == '__main__':
    main(sys.argv)
