#!/usr/bin/env python
import sys
import threading
import time

import cv2 as cv
import numpy as np
import rospy
from centermask.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError
# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, CompressedImage, RegionOfInterest, CameraInfo
import pyrealsense2 as rs2

from detectron2_ros.msg import PersonPositions, PersonPosition

# For sync depth and rgb
import message_filters


# Needed to update backbone registry
import centermask.modeling.backbone

class Centermask2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        pos = PersonPositions()

        self._bridge = CvBridge()
        self._last_rgb_msg = None
        self._last_depth_msg = None
        self._msg_lock = threading.Lock()
        self._image_counter = 0

        self.score_thresh = 0.65
        self.removal_classes = ['person', 'chair']

        self.cfg = get_cfg()
        self.cfg.merge_from_file(self.load_param('~config'))
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param('~detection_threshold') # set threshold for this model
        self.cfg.MODEL.WEIGHTS = self.load_param('~model')
        self.predictor = DefaultPredictor(self.cfg)
        self._class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get("thing_classes", None)

        self._visualization = self.load_param('~visualization',True)
        self._result_pub = rospy.Publisher('~result', Result, queue_size=1)
        self._vis_pub = rospy.Publisher('~visualization', Image, queue_size=1)
        self._static_depth_img_pub = rospy.Publisher('~static_depth_image', Image, queue_size=1)

        self._person_pos_pub = rospy.Publisher('~person_positions', PersonPositions, queue_size=1)

        # self._depth_pub = rospy.Publisher('/depth_pub', Image, queue_size=1)
        # self._rgb_pub = rospy.Publisher('/rgb_pub', Image, queue_size=1)

        # self.getIntrinsics()
        self.getManualHSRIntrinsics()


        self._image_sub = message_filters.Subscriber(self.load_param('~rgb_input'), CompressedImage)
        self._depth_sub = message_filters.Subscriber(self.load_param('~depth_input'), CompressedImage)

        self.ts = message_filters.ApproximateTimeSynchronizer([self._image_sub, self._depth_sub], 1, 0.020) # Note 3ms usually, 50ms for compressed
        self.ts.registerCallback(self.callback_images)


        self.start_time = time.time()
        rospy.loginfo("Centermask2 Node Initialized")

    def convertPixelToPosition(self, depth_img, mask):
        # Get the pixel location and depth of person centroid
        count = (mask == 1).sum()

        y_center, x_center = np.argwhere(mask==True).sum(0)/count
        depth = np.median(depth_img[mask.astype(bool)])

        # Convert to 3D position in camera coords 
        xyz = rs2.rs2_deproject_pixel_to_point(self.intrinsics, [x_center, y_center], depth/1000.0)
                    
        person_position_msg = PersonPosition()
        person_position_msg.x = xyz[0]
        person_position_msg.y = xyz[1]
        person_position_msg.z = xyz[2]

        # return xyz
        return person_position_msg

    def getManualHSRIntrinsics(self):
        rospy.loginfo("Updating camera info")

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = 640
        self.intrinsics.height = 480
        self.intrinsics.ppx = 320.5
        self.intrinsics.ppy = 240.5
        self.intrinsics.fx = 554.3827128226441
        self.intrinsics.fy = 554.3827128226441
        self.intrinsics.model = rs2.distortion.brown_conrady


    def getIntrinsics(self):
        # Get camera info
        rospy.loginfo("Updating camera info")
        cameraInfo = rospy.wait_for_message('~camera_info', CameraInfo, 60)

        self.intrinsics = rs2.intrinsics()
        self.intrinsics.width = cameraInfo.width
        self.intrinsics.height = cameraInfo.height
        self.intrinsics.ppx = cameraInfo.K[2]
        self.intrinsics.ppy = cameraInfo.K[5]
        self.intrinsics.fx = cameraInfo.K[0]
        self.intrinsics.fy = cameraInfo.K[4]
        self.intrinsics.model = rs2.distortion.brown_conrady

    def run(self):

        if self._msg_lock.acquire(False):
            img_msg = self._last_rgb_msg
            depth_msg = self._last_depth_msg
            self._last_rgb_msg = None
            self._last_depth_msg = None
            self._msg_lock.release()
        else:
            return

        if img_msg is not None:
            # dur = depth_msg.header.stamp - img_msg.header.stamp
            # print("Time diff is {0} seconds".format(abs(dur.to_sec())))

            self._image_counter = self._image_counter + 1
            if (self._image_counter % 11) == 10:
                rospy.loginfo("Images detected per second=%.2f",
                                float(self._image_counter) / (time.time() - self.start_time))

            np_image = self._bridge.compressed_imgmsg_to_cv2(img_msg)
            np_depth_image = self.convert_compressed_to_cv_image(depth_msg)

            outputs = self.predictor(np_image)
            result = outputs["instances"].to("cpu")
            result_msg = self.getResult(result)

            self._result_pub.publish(result_msg)
            
            # Mask out people with score greater than threshold
            class_ids = result.pred_classes if result.has("pred_classes") else None
            class_names = np.array(self._class_names)[class_ids.numpy()]        
            retain_inds = (result.scores.numpy() > self.score_thresh) & (class_names == 'person')
            result = result[retain_inds]

            # For each mask
            if sum(retain_inds) > 0:
                all_person_positions_msg = PersonPositions()
                all_person_positions_msg.header = depth_msg.header
                masks = np.asarray(result.pred_masks)
                for i in range(sum(retain_inds)):
                    mask = masks[i]
                    person_position_msg = self.convertPixelToPosition(np_depth_image, mask)
                    all_person_positions_msg.positions.append(person_position_msg)
                    self._person_pos_pub.publish(all_person_positions_msg)
                    # print("Person {0} at {1}".format(i, person_pos))

            # Visualize results
            if self._visualization:
                v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)
                v = v.draw_instance_predictions(result)
                img = v.get_image()[:, :, ::-1]

                image_msg = self._bridge.cv2_to_imgmsg(img)
                self._vis_pub.publish(image_msg)
                # self._depth_pub.publish(self._bridge.cv2_to_imgmsg(np_depth_image))
                # self._rgb_pub.publish(self._bridge.cv2_to_imgmsg(np_image))

    def getResult(self, predictions):

        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._rgb_header
        result_msg.class_ids = predictions.pred_classes if predictions.has("pred_classes") else None
        result_msg.class_names = np.array(self._class_names)[result_msg.class_ids.numpy()]
        result_msg.scores = predictions.scores if predictions.has("scores") else None

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]]=255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    def getThresholdedResult(self, predictions):

        inds = np.where(predictions.scores.numpy() > self.score_thresh)[0]

        boxes = predictions.pred_boxes[inds] if predictions.has("pred_boxes") else None
        

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._rgb_header
        result_msg.class_ids = predictions.pred_classes[inds] if predictions.has("pred_classes") else None
        result_msg.class_names = np.array(self._class_names)[result_msg.class_ids.numpy()][inds]
        result_msg.scores = predictions.scores[inds] if predictions.has("scores") else None

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]]=255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    def convert_to_cv_image(self, image_msg):

        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ['rgb8', 'bgr8']:
            encoding = np.uint8
        elif image_msg.encoding.lower() == 'mono8':
            encoding = np.uint8
        elif image_msg.encoding.lower() == '16uc1':
            # For depth images        
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == '32fc1':
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(shape=(image_msg.height, image_msg.width, channels),
                            dtype=encoding, buffer=image_msg.data)

        if image_msg.encoding.lower() == 'mono8':
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == '16uc1':
            pass
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def convert_compresseddepth_to_cv_image(self, image_msg):

        # 'msg' as type CompressedImage
        depth_fmt, compr_type = image_msg.format.split(';')
        # remove white space
        depth_fmt = depth_fmt.strip()
        compr_type = compr_type.strip()
        if compr_type != "compressedDepth":
            raise Exception("Compression type is not 'compressedDepth'."
                            "You probably subscribed to the wrong topic.")

        # remove header from raw data
        depth_header_size = 12
        raw_data = image_msg.data[depth_header_size:]

        depth_img_raw = cv.imdecode(np.frombuffer(raw_data, np.uint8), cv.IMREAD_ANYDEPTH)

    def convert_compressed_to_cv_image(self, image_msg):

        if image_msg is None:
            return None
        
        # 'msg' as type CompressedImage
        depth_fmt, compr_type = image_msg.format.split(';')
        depth_fmt = depth_fmt.strip()
        compr_type = compr_type.strip()

        cv_img = np.frombuffer(image_msg.data, np.uint8)
        cv_img = cv.imdecode(cv_img, cv.IMREAD_GRAYSCALE)

        return cv_img

    def getStaticDepthImage(self, depth_image, predictions):

        static_depth_img = depth_image.copy()
        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return static_depth_img # No masks found so return the whole depth image

        class_ids = predictions.pred_classes if predictions.has("pred_classes") else None
        class_names = np.array(self._class_names)[class_ids.numpy()]
        scores = predictions.scores if predictions.has("scores") else None
        
        # arr = (class_names == 'person') & (scores.numpy() > 0.5)
        arr = (np.isin(class_names, self.removal_classes)) & (scores.numpy() > 0.6)
        inds = np.where(arr)[0]
        
        if len(inds)<1:
            return static_depth_img # No relevant masks found so return whole depth image
        
        kernel = np.ones((5,5),np.uint8)

        for i, (x1, y1, x2, y2) in enumerate(boxes[arr]):
            # static_depth_img[masks[inds[i]]]=0
            # Could be worth transforming the masks to both reduce noise and dilate
            # static_depth_img[cv.dilate(masks[inds[i]],kernel,iterations = 1) ]=0
            static_depth_img[(cv.dilate(np.float32(masks[inds[i]]),kernel,iterations = 2)).astype(bool)]=0

        return static_depth_img

    def callback_images(self, rgb_msg, depth_msg):
        rospy.logdebug("Get time-synchronised images")
        # rospy.loginfo("Get time-synchronised images")
        if self._msg_lock.acquire(False):
            self._last_rgb_msg = rgb_msg
            self._last_depth_msg = depth_msg
            self._rgb_header = rgb_msg.header
            self._depth_header = depth_msg.header
            self._msg_lock.release()

            self.run()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Centermask2] %s: %s", param, new_param)
        return new_param

def main(argv):
    rospy.init_node('centermask2_ros')
    node = Centermask2node()
    rospy.spin()

if __name__ == '__main__':
    main(sys.argv)
